{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe39c8c",
   "metadata": {},
   "source": [
    "# RAG Local con Ollama y LangChain\n",
    "Este notebook implementa un sistema RAG (Retrieval-Augmented Generation) completamente local usando:\n",
    "- `Ollama` para correr modelos de lenguaje y de embeddings\n",
    "- `LangChain` para orquestar el flujo RAG\n",
    "- `FAISS` como motor de vector store local\n",
    "- Archivos locales como fuente de información\n",
    "\n",
    "Vamos paso a paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala las dependencias necesarias (solo la primera vez)\n",
    "!pip install langchain faiss-cpu sentence-transformers openai chromadb\n",
    "# Nota: asume que Ollama ya está instalado y configurado localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f507",
   "metadata": {},
   "source": [
    "## Paso 1: Carga y preprocesamiento de documentos\n",
    "Vamos a cargar algunos archivos `.txt` desde una carpeta local llamada `docs/`.\n",
    "Luego, los dividimos en fragmentos (chunks) para alimentar al sistema de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5897a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "docs_path = Path(\"docs\")\n",
    "loaders = [TextLoader(str(f)) for f in docs_path.glob(\"*.txt\")]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Split en chunks de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Documentos cargados: {len(documents)}, Fragmentos generados: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963dae1",
   "metadata": {},
   "source": [
    "## Paso 2: Generación de Embeddings con Ollama\n",
    "Usamos un modelo local de Ollama para crear los vectores de cada chunk.\n",
    "Requiere que hayas descargado el modelo de embeddings, como `nomic-embed-text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding=embedding)\n",
    "# Guardamos en disco para futuras ejecuciones\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e55fe",
   "metadata": {},
   "source": [
    "## Paso 3: Consulta y generación de respuesta con contexto\n",
    "Le hacemos una pregunta al sistema. Usamos el vector store para buscar los fragmentos más relevantes\n",
    "y luego un modelo como `llama3` para generar la respuesta basada en esos fragmentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Cargamos el índice\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "llm = Ollama(model=\"llama3\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "question = \"¿Cuál es el concepto principal en los documentos cargados?\"\n",
    "respuesta = qa_chain.run(question)\n",
    "print(\"Respuesta generada:\", respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e2006",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "Este flujo demuestra cómo implementar RAG de forma **100% local** usando Ollama, LangChain, FAISS y archivos de texto.\n",
    "Puedes expandir esto integrando PDF, CSV, herramientas de UI o conectores con otras apps como Notion, n8n, etc."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
